% SPDX-FileCopyrightText: 2024 Lukas Zirpel <thesis+lukas@zirpel.de>
% SPDX-License-Identifier: GPL-3.0-only

\chapter{Methodology}
\label{chap:methodology}

\section{Experimental design}
\begin{itemize}
  \item Was haben wir vor?
  \item Wie können wir das erreichen?
  \item Was müssen wir dabei beachten?
\end{itemize}

\section{Testbed}
\begin{itemize}
  \item Was haben wir zusammengesteckt (Figure)
  \item Warum Hardware und nicht virtuell?
  \item Warum Nix?
\end{itemize}

\section{Scenario Selection}
\subsection{Protocols to test}
\begin{itemize}
  \item none, as a baseline
  \item WireGuard
  \item DNS tunnel (iodine)
  \item ICMP tunnel (ICMPTX)
  \item phantun
  \item Various Tor Protocols
\end{itemize}

\todo{provide reasoning why we chose these protocols specifically and describe them briefly}

\subsection{Parameters to test}
\begin{itemize}
  \item Packet loss
  \item Latency
  \item Reorder packets
  \item Duplicate packets
  \item MTU
\end{itemize}
explain why these parameters, etwas wie Tabelle, Spalten Parameter







In an attempt to save space...
The \href{https://en.wikipedia.org/wiki/Pcap}{.pcap} files only compress well if the data is not encrypted or otherwise scrambled.
The files could either be explicitly compressed with a compression tool like zstd or the compression could be done at the filesystem level, e.g. with \href{https://openzfs.org/wiki/Main_Page}{ZFS}.


Since we're only interested in measuring what happens during the actual test and not in the connection setup and teardown of iperf3, we use heuristics
The connection setup and teardown of iperf3 should not be part of the analysis, hence a heuristic is employed to ignore this part of the packet capture. To find the start of the test, we find the first packet which is larger than a threshhold. To find the end of the test, we find the last packet which is larger than a threshhold and also ignore all packets that are sent after the duration of the test is over.

\href{https://en.wikipedia.org/wiki/Maximum_transmission_unit}{MTU}:
When the MTU inside of a tunnel is so large, that the overhead of the tunnel protocol would make the packet too big to be transported over the internet
Needs to be dealt with in some way. Usually by either dropping the packet and/or communicating the error to the application (\href{https://en.wikipedia.org/wiki/Path_MTU_Discovery}{Path MTU discovery}) or by fragmenting the packet. Since fragmentation has a relatively large performance overhead, this is usually avoided in practice. For this reason, we avoid fragmentation in this research by carefully choosing the MTU inside of the tunnels.

\todo{go through presentation and extract the info here}

\href{https://github.com/nix-community/nixos-anywhere}{nixos-anywhere} is used to quickly and reproducibly install the operating system on all machines.
nixos-anywhere uses \href{https://github.com/nix-community/disko}{disko} to partition and format the disks.


\todo{Define pre and post to mean before and after the router}

Data analysis pipeline:
\begin{itemize}
  \item run the experiment setup
  \item capture packets before and after the router and store them in .pcap files
  \item for each packet, extract the timestamp and compute the size and \href{https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE3}{BLAKE3} hash of the IP payload and write the information into a \href{https://en.wikipedia.org/wiki/JSON}{JSON} file (done in analysis/parse/parse.py)
  \item for each packet captured before the router, find the same packet after the router (done in analysis/statistics/statistics.py). If we assume, that all packets are unique and that the router does not modify packets, then we can match up the unique hash of each packet before the router to some number of packets after the router. If no matching packet can be found after flowing through the router, it means that the packet was dropped. If more than one packet matches, it means that it was duplicated. If exactly one packet matches, it was routed without duplication or being dropped. For meaningful statistics, we group the data into short chunks (buckets) (1s) and compute bandwidth and packet counts over that bucket. The latency introduced by the router can be measured by computing the difference between the timestamps of each packet before the router and the first corresponding packet after the router. Dropped packets are ignored and for duplicated packets only the first arriving packet is counted. Throughput can be measured by summing up the IP payload \todo{should we measure at a different layer?} and dividing it by the time duration of the bucket. Dropped packets are ignored and packets which were duplicated count only once. The timestamp, dropped, duplicated and normal packet counts, throughput and a list of the latency of each packet are stored per bucket and written to a JSON file.
  \item Graphs are drawn using \href{https://matplotlib.org/}{Matplotlib} (done in analysis/graph/graph.py)
  \item \todo{for gaining insights, for example while comparing two different scenarios, it's helpful to plot data from multiple experiments in one plot. This still needs to be implemented.}
\end{itemize}


\todo{explain why we're capturing packets on a separate host.}
In the VM test setup, the virtual switch is configured to behave like a \href{https://en.wikipedia.org/wiki/Ethernet_hub}{hub} (\href{https://github.com/NixOS/nixpkgs/blob/0634959ae9c75ac8cab28dfcc9a0f045cf30dfc6/nixos/lib/test-driver/test_driver/vlan.py#L43}{\todo{reference to this}}), which makes it easy to capture the packets on both VLANs. On real hardware with a real switch, this is not quite as trivial. To replicate this setup with real hardware, we use Cisco's Switched Port Analyzer (SPAN), also known as port mirroring. Since enabling SPAN disables our ability to transmit packets from that interface for management purposes, we use a USB to Ethernet adapter for the management interface.
\todo{Specify the exact hardware model of the PCs and the SBCs}
