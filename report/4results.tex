% SPDX-FileCopyrightText: 2024 Lukas Zirpel <thesis+lukas@zirpel.de>
% SPDX-License-Identifier: GPL-3.0-only

\chapter{Results}
\label{chap:results}
\todo{beschreiben, was man sieht, Messungen irgendwie anordnen, innerhalb Sektion mit anderem vergleichen, z.B. MTU. zuerst nach Protokollen, dann Parameter}

In this section, we describe the results obtained by performing our
experiments.

\todo[inline]{Move the following into the Discussuion section}
\section{RQ1: How much packet size overhead does each protocol add?}
While we aimed to quantify the precise packet size overhead introduced by each protocol, our experimental setup did not allow for direct measurement of this metric.
However, it is important to note that the packet size overheads of many protocols are generally constant.
This overhead is due to the protocol's header, which is added to the data payload.
\todo[inline]{break down overhead of WireGuard and ICMPTX}

\section{RQ2: How much does the MTU decrease by using each protocol?}
For protocols that do not support fragmentation by themselves, the MTU decrease is directly related to the protocol's overhead.

\section{RQ3: How much additional latency does each protocol add?}
Unfortunately, the measurement setup was not designed correctly to accurately capture the additional latency introduced by each protocol.
Measuring additional latency introduced by a protocol requires observing the packets before they enter the tunnel and after they exit it.
The additional latency introduced by the protocol can then be analyzed by measuring how long each packet takes on average and subtracting the latency when no protocol is being used.
See \cref{fig:optimal_network_schematic} for a setup, which would be up to the task.
While we acknowledge the importance of latency as a performance metric, we are unable to reliably quantify it within the constraints of our current experimental design.

\section{RQ4: Do any protocols introduce additional packet loss?}
Similarly to latency measurements, our setup was not equipped to reliably measure packet loss introduced by the protocols themselves.



\section{RQ5: How much processing power and RAM does each protocol consume/require?}
did not measure

for userspace tunnel programs, can be measured using systemctl status

We are not aware of a good method of measuring the RAM usage of WireGuard.

We determine the CPU usage of WireGuard by looking at the output of \texttt{...} during a measurement.

The CPU usage is fairly constant over time and on both sides of the tunnel.

WireGuard used approximately \% of one CPU core



graphen beschreiben (Welche Achsen gibt es, etc.)
was sieht man?
warum sieht man das was man sieht?


graphs:
latency
packet counts
throughput

single or multi

single: axes:

for looking at a specific measurement in detail:

gives an overview of how consistent the measurement is

latency: box plot for every bucket: time, latency

packet counts: line with dot every bucket: time, counts

throughput: line with dot every bucket: time, throughput


to be able to see the influence of a parameter, multiple measurements in one graph:

\begin{figure}[tbh]
	\centering
	\includegraphics[draft=false,width=0.9\textwidth]{figures/Graphs/graph-1-mtu/latencies.pdf}
	\caption{Latencies for varying payload sizes}\label{fix:graph-1-mtu-latencies}
\end{figure}
Latency stays pretty much constant, with different packet sizes

only the tail latency is increasing drastically, which is unexpected.

\begin{figure}[tbh]
	\centering
	\includegraphics[draft=false,width=0.9\textwidth]{figures/Graphs/graph-1-mtu/throughput.pdf}
	\caption{Throughput for varying payload sizes}\label{fix:graph-1-mtu-throughput}
\end{figure}

The different measurements are always plotted on the x axis.

latency: violin plot: measurement, latency

packet duplicate: violin plot: measurement, ratio duplicate to total sent

packet dropped: violin plot: measurement, ratio dropped to total sent

throughput (with and without overhead): violin plot: measurement, throughput


move to chapter methodology:
one second buckets


We were unable to evaluate ICMPTX.
The program has a bug, where it crashes almost immediately after iperf3 starts sending data through the tunnel.
It fails with the error message \texttt{sendto: No buffer space available}.
The issue was mentioned previously \cite{icmptx-sendto-no-buffer-space-avaiable}.
The maintainer proposed two workarounds but using either one alone or both at the same time did not resolve the issue.
The problem seems to occur when trying to send data into the tunnel at a faster rate than the underlying link can support.
Tunnel software should be able to handle this case by dropping packets instead of crashing.


iodine

only one packet can be in flight, so usable throughput depends on latency and is generally very low

seems to not be true, since observed data rate was higher than expected

perhaps only the case in one direction?

Can not be measured reliably with our setup, would need improved setup, see figure



mostly see what we expected

we measure exactly the latency, loss and duplication we told the network emulator to simulate

measured throughput is influenced by number of dropped packets


Tor pluggable transports like obfs4 and Snowflake
meant for stream data like TCP, not UDP
\todo[inline]{Verify this claim more thouroughly!}

since we're only sending UDP packets, cannot be measured

would need a different test setup with a connection oriented data stream to test

instead of packet oriented

or some kind of UDP over TCP tunnel

this would also be influenced by congestion control algorithm


Rather than only a handful of combinations of parameters, as described in \cref{chap:methodology}, we tested many more.
In the beginning we even tested the cartesian product of all parameters.
This fact combined with several slightly different versions of our test setup, lead us to collect about 5 TB of packet captures, which compress to a little more than half of that using the zstd compression algorithm in Btrfs.
