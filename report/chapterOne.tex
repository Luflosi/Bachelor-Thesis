% SPDX-FileCopyrightText: 2024 Lukas Zirpel <thesis+lukas@zirpel.de>
% SPDX-License-Identifier: GPL-3.0-only

\chapter{Introduction}


Many places in the world encounter internet censorship. To try to get around the censorship, many different techniques, some very creative, were developed to obfuscate data and hide it from the censors or make it seem like innocent data. These censorship circumvention protocols are probably developed in laboratory conditions with ideal networks and plentiful computational resources. These are not the same conditions encountered in the real world, where networks may have a high latency and low throughput. Running these protocols on phones or other mobile devices also constrains the available CPU and RAM resources.



In an attempt to save space...
The \href{https://en.wikipedia.org/wiki/Pcap}{.pcap} files only compress well if the data is not encrypted or otherwise scrambled.
The files could either be explicitly compressed with a compression tool like zstd or the compression could be done at the filesystem level, e.g. with \href{https://openzfs.org/wiki/Main_Page}{ZFS}.


Since we're only interested in measuring what happens during the actual test and not in the connection setup and teardown of iperf3, we use heuristics
The connection setup and teardown of iperf3 should not be part of the analysis, hence a heuristic is employed to ignore this part of the packet capture. To find the start of the test, we find the first packet which is larger than a threshhold. To find the end of the test, we find the last packet which is larger than a threshhold and also ignore all packets that are sent after the duration of the test is over.

\href{https://en.wikipedia.org/wiki/Maximum_transmission_unit}{MTU}:
When the MTU inside of a tunnel is so large, that the overhead of the tunnel protocol would make the packet too big to be transported over the internet
Needs to be dealt with in some way. Usually by either dropping the packet and/or communicating the error to the application (\href{https://en.wikipedia.org/wiki/Path_MTU_Discovery}{Path MTU discovery}) or by fragmenting the packet. Since fragmentation has a relatively large performance overhead, this is usually avoided in practice. For this reason, we avoid fragmentation in this research by carefully choosing the MTU inside of the tunnels.

TODO: why Nix?
TODO: convert from Markdown to proper LATEX
TODO: go through presentation and extract the info here

\href{https://github.com/nix-community/nixos-anywhere}{nixos-anywhere} is used to quickly and reproducibly install the operating system on all machines.
nixos-anywhere uses \href{https://github.com/nix-community/disko}{disko} to partition and format the disks.


Thanks to Jacek Galowicz for helping me make the VM tests scale to thousands of tests without running out of memory and with great performance.

TODO: Define pre and post to mean before and after the router

Data analysis pipeline:
- run the experiment setup
- capture packets before and after the router and store them in .pcap files
- for each packet, extract the timestamp and compute the size and \href{https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE3}{BLAKE3} hash of the IP payload and write the information into a \href{https://en.wikipedia.org/wiki/JSON}{JSON} file (done in analysis/parse/parse.py)
- for each packet captured before the router, find the same packet after the router (done in analysis/statistics/statistics.py). If we assume, that all packets are unique and that the router does not modify packets, then we can match up the unique hash of each packet before the router to some number of packets after the router. If no matching packet can be found after flowing through the router, it means that the packet was dropped. If more than one packet matches, it means that it was duplicated. If exactly one packet matches, it was routed without duplication or being dropped. For meaningful statistics, we group the data into short chunks (buckets) (1s) and compute bandwidth and packet counts over that bucket. The latency introduced by the router can be measured by computing the difference between the timestamps of each packet before the router and the first corresponding packet after the router. Dropped packets are ignored and for duplicated packets only the first arriving packet is counted. Throughput can be measured by summing up the IP payload (TODO: should we measure at a different layer?) and dividing it by the time duration of the bucket. Dropped packets are ignored and packets which were duplicated count only once. The timestamp, dropped, duplicated and normal packet counts, throughput and a list of the latency of each packet are stored per bucket and written to a JSON file.
- Graphs are drawn using \href{https://matplotlib.org/}{Matplotlib} (done in analysis/graph/graph.py)
- TODO: for gaining insights, for example while comparing two different scenarios, it's helpful to plot data from multiple experiments in one plot. This still needs to be implemented.


TODO: explain why we're capturing packets on a separate host.
In the VM test setup, the virtual switch is configured to behave like a \href{https://en.wikipedia.org/wiki/Ethernet_hub}{hub} (\href{https://github.com/NixOS/nixpkgs/blob/0634959ae9c75ac8cab28dfcc9a0f045cf30dfc6/nixos/lib/test-driver/test_driver/vlan.py#L43}{TODO: reference to this}), which makes it easy to capture the packets on both VLANs. On real hardware with a real switch, this is not quite as trivial. To replicate this setup with real hardware, we (TODO: correct form?) use Cisco's Switched Port Analyzer (SPAN), also known as port mirroring. Since enabling SPAN disables our ability to transmit packets from that interface for management purposes, we use a USB to Ethernet adapter for the management interface.

%%% Local Variables:
%%% TeX-master: "thesis"
%%% End:
